---
permalink: /overview/
---
# Overview

## AI and Society

Rapid [progress](http://realai.org/progress/) in artificial intelligence (AI) has raised the prospect that machines will [one day](http://realai.org/timing/) surpass humans in intelligence. Such AI systems are referred to as “general AI”, “strong AI”, or AGI ([artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence)). Countries have [policies](http://realai.org/policies/) in place that support AGI research and development. Capable [research groups](http://realai.org/labs/) are well-funded, some directly sponsored by the technology [industry](http://realai.org/industry/). Several AGI [roadmaps](http://realai.org/roadmaps/) have already been published by renowned experts.

The creation of AGI could mark the beginning of [technological singularity](https://en.wikipedia.org/wiki/Technological_singularity), and represent the greatest change in the history of life on Earth. It could bring enormous benefits to humanity, but could also spell disaster if not properly developed. Given its imminence and potential [impacts](http://realai.org/impacts/), the development of AGI will affect all of us, and deserves a lot more attention from the wider society.

The best place to appreciate AI’s progress is at its research [frontiers](http://realai.org/frontiers/). Unlike mature fields where many years of postgraduate training is needed, the [background](http://realai.org/resources/curriculum/) for cutting-edge AI is accessible to college students, whose papers often appear in top academic conferences.

We can take steps to ensure that the creation of AGI is [safe and beneficial](http://realai.org/safety/). To [positively shape AGI's development](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/), things that can be done today include research, policy, advocacy and capacity building. Many [AI safety organizations](http://realai.org/safety/organizations/) around the world can [effectively](http://realai.org/safety/effective-altruism/) put additional human and financial resources to good use. For those who feel the urgency or are willing to contribute, there are many ways, from small donations to lifetime [career changes](https://80000hours.org/). With hard work and caution, we believe that when singularity comes, it's not going to be a disaster. It's going to be the next level of civilization. In this new world, everyone can live well and prosper.

### Near-Term Safety

In a widely cited article about the singularity, [Vinge (1993)](http://edoras.sdsu.edu/~vinge/misc/singularity.html) predicts that greater-than-human intelligence will occur during the next 30 years. In this website, near-term safety is about keeping AGI safe and beneficial if it will be created by the end of 2023. A plausible path of such a development is the [Prosaic AI](http://realai.org/prosaic/) scenario, in which we can scale up existing [deep learning](https://en.wikipedia.org/wiki/Deep_learning) methods to build AGI, without the help of qualitatively new ideas about how intelligence works.

## References

* 1993, Vernor Vinge. [The Coming Technological Singularity: How to Survive in the Post-Human Era](http://edoras.sdsu.edu/~vinge/misc/singularity.html). *VISION-21 Symposium*.

