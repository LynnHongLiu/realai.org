{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](http://realai.org/) > [Course](http://realai.org/course/) > [TensorFlow](http://realai.org/course/tensorflow/) > [RNN](http://realai.org/course/tensorflow/#rnn) >\n",
    "\n",
    "# Character RNN\n",
    "\n",
    "*Last Updated: September 9, 2017*\n",
    "\n",
    "The [GPU](http://realai.org/course/tensorflow/#gpu) VM built from the last session allows us to explore more interesting neural networks within a reasonable time. In this session, we use TensorFlow to build a recurrent neural network (RNN) on English characters. This model is widely known for its [unreasonable effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "\n",
    "First we list all the necessary modules and constants. Although they appear at the very beginning, they're actually collected over time as the code is gradually developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Model parameters\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "NUM_STEPS = 32\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 4\n",
    "\n",
    "# Testing parameters\n",
    "INFERENCE_LEN = 200\n",
    "TEMPERATURE = 2.0\n",
    "\n",
    "# Output parameters\n",
    "INFER_INTERVAL = 1000\n",
    "LOGDIR = \"/tmp/char_RNN\"\n",
    "\n",
    "# Data parameters\n",
    "FILE_PATH = \"data/shakespeare.txt\"\n",
    "STRIDE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All training data are characters contained in a text file. The file `shakespeare.txt` used here is [downloaded](https://github.com/hzy46/Char-RNN-TensorFlow/raw/master/data/shakespeare.txt) from a nicely implemented [GitHub project](https://github.com/hzy46/Char-RNN-TensorFlow) of character RNNs. Its first 10,000 lines are collected in `shakespeare_10K.txt`, a smaller file more convenient for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/:\r\n",
      "shakespeare_10K.txt  shakespeare.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls -R data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each unique character in the entire file is encoded as an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read character file into a huge string\n",
    "with open(FILE_PATH, 'r') as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "# Encoder and decoder for characters\n",
    "char_set = set(raw_data)\n",
    "\n",
    "encode_dict = {}\n",
    "decode_dict = {}\n",
    "size_charset = 0\n",
    "\n",
    "for char in char_set:\n",
    "    encode_dict[char] = size_charset\n",
    "    decode_dict[size_charset] = char\n",
    "    size_charset += 1\n",
    "\n",
    "# Encode raw data into data\n",
    "data_len = len(raw_data)\n",
    "data = np.zeros(data_len)\n",
    "\n",
    "for i in range(data_len):\n",
    "    data[i] = encode_dict[raw_data[i]]\n",
    "\n",
    "del(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batches will be generated from the data of encoded characters. They will be rearranged at the beginning of each epoch to allow easy generation of input and target. Target has the same shape as input. The target character is the character next to the corresponding input character in the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a data generator for each epoch\n",
    "def all_batches():\n",
    "    for start in range(0, batch_len-NUM_STEPS, STRIDE):\n",
    "        yield (all_batches_data[:, start:(start + NUM_STEPS)], \n",
    "               all_batches_data[:, (start + 1):(start + NUM_STEPS + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) of cells that belong to the class [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell), which is composed sequentially of [`tf.contrib.rnn.BasicLSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell). In plain English, it's a multi-layer LSTM model whose computational details are conveniently packaged in TensorFlow modules, so that we can just \"describe\" them in a few lines of code.\n",
    "\n",
    "During training, inputs to the model are encoded characters of shape [BATCH_SIZE, NUM_STEPS]. Testing is conducted online where we take a single character in shape [1, 1] to bootstrap inference. To meet the need of both training and testing, our computation graph begins with an input of shape [None, None]. A new inner-most axis is then added by [`tf.one_hot`](https://www.tensorflow.org/api_docs/python/tf/one_hot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Input_Characters\"):\n",
    "    encoded_input = tf.placeholder(tf.int32, (None, None), name=\"Encoded_Input\")\n",
    "    one_hot_input = tf.one_hot(encoded_input, size_charset, name=\"One_Hot_Input\")\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell(\n",
    "    [tf.contrib.rnn.BasicLSTMCell(HIDDEN_SIZE) for _ in range(NUM_LAYERS)]\n",
    ")\n",
    "\n",
    "initial_state = cell.zero_state(tf.shape(one_hot_input)[0], tf.float32)\n",
    "outputs, out_state = tf.nn.dynamic_rnn(cell, one_hot_input, initial_state=initial_state)\n",
    "\n",
    "# Output logits\n",
    "logits = tf.layers.dense(outputs, size_charset, activation=None, name=\"Logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Adam optimizer to minimize cross entropy loss with the target characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Target_Characters\"):\n",
    "    encoded_target = tf.placeholder(tf.int32, (None, NUM_STEPS), name=\"Encoded_Target\")\n",
    "    target = tf.one_hot(encoded_target, len(char_set), name=\"One_Hot_Target\")\n",
    "\n",
    "# Loss, training and sampling\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=logits),\n",
    "        name=\"Mean\")\n",
    "\n",
    "with tf.name_scope(\"Optimizer\"):\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.001, name=\"Adam\").minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TEMPERATURE` is a hyperparameter that controls how confidently we'd like our model to behave. Lower temperature exaggerates the differences among logits and causes preferred characters to be chosen by [`tf.multinomial`](https://www.tensorflow.org/api_docs/python/tf/multinomial) with overwhelming higher probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Sample\"):\n",
    "    sample = tf.multinomial(tf.exp(logits[:, -1] / TEMPERATURE), 1)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the computation graph\n",
    "if tf.gfile.Exists(LOGDIR):\n",
    "    tf.gfile.DeleteRecursively(LOGDIR)\n",
    "tf.gfile.MakeDirs(LOGDIR)\n",
    "\n",
    "writer = tf.summary.FileWriter(LOGDIR, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the computation graph is complete and looks like\n",
    "\n",
    "![](http://realai.org/course/tensorflow/char-RNN-1.png)\n",
    "\n",
    "Testing is conducted by running a partially trained model many steps from a random starting character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    # Initialization: one-character input and initial RNN state\n",
    "    new_index = [np.random.randint(0, size_charset)]\n",
    "    new_input = [new_index]\n",
    "    print(decode_dict[new_index[0]], end='')\n",
    "    new_state = sess.run(initial_state, feed_dict={encoded_input: new_input})\n",
    "\n",
    "    for i in range(INFERENCE_LEN - 1):\n",
    "        new_index, new_state = sess.run([sample, out_state], feed_dict={encoded_input: new_input, initial_state: new_state})\n",
    "        new_input = [new_index]\n",
    "        print(decode_dict[new_index[0]], end='')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full training using a cloud [GPU](https://cloud.google.com/compute/pricing#gpus) on an n1-standard-2 (2 vCPUs, 7.2 GB memory) machine should take less than 15 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Epoch 0 runs in 4466 steps *****\n",
      "Step 1000 training loss is 2.47922. Time: 41.00s\n",
      "HnW Voppan pothe me bereAd the the anle the beresG\n",
      "Thep more the the cout the ther the LoHn jorithe the son the me the soud he the the wond me the sorge fo gore borens the the me founs the the coreeut\n",
      "\n",
      "Step 2000 training loss is 2.27208. Time: 41.09s\n",
      "be the gound Xe the sor in the core and Bather the the reand the QoThe gour the colle so of me the we she the in role head he of she the so le so the the the the and she be not and the the the and the\n",
      "\n",
      "Step 3000 training loss is 2.14611. Time: 40.62s\n",
      "well and the the the cound in the cond the be the me your will he sore heI so gore not you hear in with her the with of and the the wistary I all the the goo he the condering sone me deded so and her \n",
      "\n",
      "Step 4000 training loss is 2.10461. Time: 40.42s\n",
      "id here [orter dow me the will and and and courd in here the wist ;\n",
      "Here the $all me with me sim be the locke, spare shat the sand the your the mare the loth me will the will the mare the come the and\n",
      "\n",
      "End of epoch 0 training loss is 2.01428\n",
      ".\n",
      "\n",
      "PERTINIA:\n",
      "I have shat he spather will Frong you it grome me in Frother the the dond the with the mest the prother the is I hat the courd us the not for the mand she come of in the and the heres and\n",
      "\n",
      "***** Epoch 1 runs in 4466 steps *****\n",
      "Step 1000 training loss is 2.01015. Time: 39.98s\n",
      ".\n",
      "\n",
      "LATIUS:\n",
      "The have the with the mant the mane grome the the mand sand the mand the 'tree the come the the preartert with with the sour have the have be the sone the mant come the manter be beon the A\n",
      "\n",
      "Step 2000 training loss is 1.98246. Time: 40.53s\n",
      "[le so with of hery sore sing of here with the beat.\n",
      "\n",
      "SILIN:\n",
      "I pround of the have do is the doter enteres not the manther so for the mant\n",
      "He the and the beat the have the have bettert me shall me the \n",
      "\n",
      "Step 3000 training loss is 1.86706. Time: 41.04s\n",
      "e the manper be of Rome the man the with and Carning and the the 3\n",
      "This a come and the man the better simTher the searter Jown the dood the know the man my lord,\n",
      "The know the man her and man of my son\n",
      "\n",
      "Step 4000 training loss is 1.93106. Time: 40.79s\n",
      "ere your the have God be and the preates Jof me the doUnding and with the bet[en of the such the with ender and not the down me hear the sirn the concend the mands and the beard of the better and in h\n",
      "\n",
      "End of epoch 1 training loss is 1.85039\n",
      "ke the be&d and with in the have do of the preath and the better the some and the aton better the love of the reation the sone But With the gost with Jull the contertent and bece the and the preat of \n",
      "\n",
      "***** Epoch 2 runs in 4466 steps *****\n",
      "Step 1000 training loss is 1.82970. Time: 40.11s\n",
      "MELLAN:\n",
      "The with a fainters me, in the bear in the manEst me sir the lord the prace in the sirved restreet the sear your the fair the man up  a prove beuse of quear and prought the With the ear\n",
      "The be\n",
      "\n",
      "Step 2000 training loss is 1.83921. Time: 40.75s\n",
      "the stall the for&s and speak and thee man and the have soble of the preat of with her hece shall speak I come with me me her the parter:\n",
      "I shall with with of the dook with me the Xare the beat the be\n",
      "\n",
      "Step 3000 training loss is 1.74458. Time: 40.47s\n",
      "OLIUS:\n",
      "What he shall me my soin of the some ?\n",
      "\n",
      "CARIANA:\n",
      "A will stall and with the stall and the grace the man the man in the come the king the love in the better me the sing for my sond rest for the r\n",
      "\n",
      "Step 4000 training loss is 1.80080. Time: 40.48s\n",
      "I and her with the not the the such and the great me to your speak the brought love the shall sir the preather and gratess the are in the great the sing of the such the fortune and me.\n",
      "\n",
      "CASSIANUS:\n",
      "The\n",
      "\n",
      "End of epoch 2 training loss is 1.75314\n",
      "ke me is some of not the sumper\n",
      "The stainst the with in the sing of the questy becentle your do the xangerpent,\n",
      "The good me so me the man the prought the prought the king the the hands and sainted of \n",
      "\n",
      "***** Epoch 3 runs in 4466 steps *****\n",
      "Step 1000 training loss is 1.77441. Time: 40.05s\n",
      "Xo[ce shall I come the great the shall the speak of the with the hand and fair the sa be bear so,\n",
      "The man the brown soble prother with the bear the hand not here here me not the with the bear the with\n",
      "\n",
      "Step 2000 training loss is 1.77056. Time: 40.65s\n",
      "be the such of the now the forged the heart the prought and the sue\n",
      "should been the manst the with the best the beart of I shall with the lord,\n",
      "And the prinest me the place and I have man the sing of \n",
      "\n",
      "Step 3000 training loss is 1.67544. Time: 40.69s\n",
      "fGe[\n",
      "The bear the shall love the earth me the destremble so England of the courting of the such been your such and been the that my Rome.\n",
      "\n",
      "PRINCE HENRY:\n",
      "What he is the prace the seems of a prace the p\n",
      "\n",
      "Step 4000 training loss is 1.76376. Time: 40.34s\n",
      "Or the brother ,\n",
      "The man for the sear in the sing the care and be.\n",
      "\n",
      "CLEOPATRA:\n",
      "I have be a read the man be of the heart.\n",
      "\n",
      "CLAUDIO:\n",
      "And the your dostion of the brown the courted the strange in the cour\n",
      "\n",
      "End of epoch 3 training loss is 1.69643\n",
      "come and with the for the courted for the shall so soH Antoning mean and his some come shall so so heart the world and with xight the offer[sI so the stanger of enought the counters,\n",
      "The with the shal\n",
      "\n",
      "CPU times: user 15min 37s, sys: 47.5 s, total: 16min 25s\n",
      "Wall time: 12min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for e in range(NUM_EPOCHS):\n",
    "    \n",
    "    # Set up batched character input data from random start\n",
    "    seed = np.random.randint(0, NUM_STEPS)\n",
    "    batch_len = (data_len - seed) // BATCH_SIZE\n",
    "    all_batches_data = np.reshape(data[seed:(seed + BATCH_SIZE*batch_len)], (BATCH_SIZE, batch_len))\n",
    "        \n",
    "    print(\"***** Epoch {} runs in {} steps *****\".format(e, (batch_len-NUM_STEPS-1) // STRIDE + 1))\n",
    "    start = time.time()\n",
    "    step_counter = 0\n",
    "    for batch in all_batches():\n",
    "        # The training step\n",
    "        Loss, _ = sess.run((loss, train), feed_dict={encoded_input: batch[0], encoded_target: batch[1]})\n",
    "        \n",
    "        step_counter += 1\n",
    "        if step_counter % INFER_INTERVAL == 0:\n",
    "            print(\"Step {} training loss is {:.5f}. Time: {:.2f}s\".format(step_counter, Loss, time.time() - start))\n",
    "            start = time.time()\n",
    "            run_inference()\n",
    "        \n",
    "    print(\"End of epoch {} training loss is {:.5f}\".format(e, Loss))\n",
    "    run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
