---
permalink: /deep-learning/implementation/
redirect_from: /DL/implementation/
title: DL Implementation
---
# Deep Learning Implementation

## System

A typical approach to address the computational requirements of large-scale neural networks is to use a heterogeneous distributed environment with a mixture of many CPUs and GPUs. [Mirhoseini & Pham et al. (2017)](https://arxiv.org/abs/1706.04972) use reinforcement learning to optimize device placement for TensorFlow computational graphs and Ô¨Ånd non-trivial device placements for Inception-V3 and RNN LSTM.

## Hardware

* 2017 April 5, Norm Jouppi. [Quantifying the performance of the TPU, our first machine learning chip](https://cloudplatform.googleblog.com/2017/04/quantifying-the-performance-of-the-TPU-our-first-machine-learning-chip.html). *Google Cloud Platform Blog*.

## Benchmarks

* 2017 September 14, Yang You, Zhao Zhang, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. [ImageNet Training in 24 Minutes](https://arxiv.org/abs/1709.05011). *arXiv:1709.05011*.

## References

* 2017 June 13, Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. [Device Placement Optimization with Reinforcement Learning](https://arxiv.org/abs/1706.04972). *arXiv:1706.04972*.
* 2016 April 22, Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. [Benchmarking Deep Reinforcement Learning for Continuous Control](https://arxiv.org/abs/1604.06778). *arXiv:1604.06778*.
